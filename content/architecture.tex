\chapter{Architecture}
\label{ch:architecture}

In this chapter, the architecture of the \ac{WSI} and \ac{WSD} system is 
presented, as well as its 
articulation with the \ac{STRING} \ac{NLP} system.

The architecture of this system is composed of four components:

\begin{enumerate}
  \item A corpus pre-parser, which prepares the corpus being used to be
processed by \ac{STRING};
  \item The co-occurrence extractor from \citep{correia2015syntax};
  \item A graph constructor and clustering algorithm;
  \item A sense disambiguation module.
\end{enumerate}

The data flow is represented in Figure~\ref{fig:data-progression}. The 
processed text from \ac{STRING} 
goes through a modified implementation of the co-occurrence extractor from 
\citep{correia2015syntax}, which stores the co-occurrence word-pairs along with 
their frequencies and association measure values in a database.

\begin{figure}[ht]
 \caption{The progression of data as it evolves through the architecture}
 \label{fig:data-progression}
 \centering
 \include{graphics/data-progression}
\end{figure}

\section{Co-occurrence Extraction and Storage}

Syntactic co-occurrences are extracted using the work by
\citet{correia2015syntax}. Additionally, this work uses the named entities
obtained from \ac{XIP}. If the word in a co-occurrence is a named entity, the
lemma of the word is attributed with the appropriate category (for example, the
word \emph{Pedro} is identified with the lemma \texttt{INDIVIDUAL}).

The co-occurrences are then stored in an SQLite database, using the \ac{ER} model in
Figure~\ref{fig:er-model}, adapted from \citet{correia2015syntax}, with minor 
changes to keep information of all the sentences used (in the \emph{Context}
entity) instead of only 20 randomly selected sentences for each co-occurrence.

\begin{figure}[ht]
    \caption{The \acl*{ER} model used to store the information in the database}
    \label{fig:er-model}
    \centering
    \include{graphics/er-model}
\end{figure}

Additionally, the relationship \emph{Exemplifies} is replaced with
\emph{Occurs}, which associates all \emph{Co-occurrence} aggregations with all
the respective \emph{Context}s where they occur.

\section{Graph Generation and Clustering}

For a target word $w$, a query to the database is made to obtain the word pairs 
and association measure values of all co-occurrences which occur in the same 
contexts as $w$. After filtering all co-occurrences which do not reach the 
minimum threshold value of the association measure being used, the 
co-occurrences are saved in a graph structure.

To ensure that only words directly related to $w$ remain, a breadth-first 
search is made starting from the target word. Only the nodes which were visited 
during this process are kept in the final graph.

After the graph is generated, a graph clustering algorithm is ran against it,
and the resulting senses are stored.

\section{Sense Disambiguation}

To disambiguate senses of a target word $w$ from a given context
$c$, the co-occurrences from $c$ are extracted and used to generate
the co-occurrence cluster for the context, $C_i$.

Then, for each inferred sense cluster $C_j$ of $w$, the
\emph{Separation} between $C_i$ and $C_j$ is calculated according to
Equation~\ref{eq:separation} \citep{hope2013uos}, in which
$\operatorname{proximity}$ is defined as the weight of the co-occurrence in the
inferred sense graph.

\begin{equation}
 \operatorname{separation}(C_i,C_j) =
 1 - \left(
 \frac {\sum_{\substack{x \in C_i \\ y \in C_j}} \operatorname{proximity}(x,y)}
       {|C_i| \times |C_j|}
 \right)
\label{eq:separation}
\end{equation}

The cluster $C_j$ with the lowest separation score compared to $C_i$ is then
considered the most likely sense of the target word $w$.


% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
