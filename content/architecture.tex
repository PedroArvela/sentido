\chapter{Architecture}
\label{ch:architecture}

The architecture of this project is composed of four components:

\begin{enumerate}
  \item A corpora pre-parser, which prepares the corpora being used to be
processed by \ac{STRING};
  \item The co-occurrence extractor from \citep{correia2015syntax};
  \item A graph constructor and clustering algorithm;
  \item A sense disambiguation module.
\end{enumerate}

The data follows the flow exemplified in Figure~\ref{fig:data-progression}. The
processed text from \ac{STRING} goes through a modified implementation of the
co-occurrence extractor from \citep{correia2015syntax}, which stores the
co-occurrences along with their frequencies and association measures in a
database.

\begin{figure}[ht]
 \caption{The progression of data as it evolves through the architecture}
 \label{fig:data-progression}
 \centering
 \include{graphics/data-progression}
\end{figure}

\section{Co-occurrence Storage}

The co-occurrences are stored in an SQLite database, using the \ac{ER} model in
Figure~\ref{fig:er-model}, adapted from \citep{correia2015syntax}, with minor
changes to keep information of all the sentences used instead of only 20
randomly selected sentences for each co-occurrence.

\begin{figure}[ht]
  \caption{The \acl*{ER} model used to store the information in the database}
  \label{fig:er-model}
  \centering
  \include{graphics/er-model}
\end{figure}

The changes in relation to \citep{correia2015syntax} are that the entity
\emph{Context} replaces \emph{Sentence}, with information about the file used to
store the information as well as the sentence number in that file.

Additionally, the relationship \emph{Exemplifies} is replaced with
\emph{Occurs}, which associates all \emph{Co-occurrence} aggregations with all
the respective \emph{Context}s where they occur.

As in \citep{correia2015syntax}, the following \ac{IC} were identified:

\begin{enumerate}
  \item The word in \emph{Co-occurrence} must belong to the \emph{Corpus} to
    which they were associated with;
  \item The \emph{Co-occurrence} association must be associated to the same
    \emph{Property} to which the words associate with in \emph{Belongs};
  \item The sentences in \emph{Context} must belong to the same \emph{Corpus}
    as the \emph{Co-occurrences} which occur in these.
\end{enumerate}

\section{Graph Generation and Clustering}

For a target word $w$, a query to the database is made to obtain all
co-occurrences which occur in the same contexts as $w$, along with the
respective association measures. After filtering all co-occurrences which do not
reach the minimum threshold value of the association measure being used, the
co-occurrences are saved in a graph structure.

To ensure only words directly related remain, a breadth-first search is made
starting from the target word. Only the nodes which were visited during this
process are kept in the final graph.

After the graph is generated, a graph clustering algorithm is ran against it,
and the resulting senses are stored.

\section{Sense Disambiguation}

To disambiguate senses of a target word $w$ from a given context
$c$, the co-occurrences from $c$ are extracted and used to generate
the co-occurrence cluster for the context, $C_i$.

Then, for each inferred sense cluster $C_j$ of $w$, the
\emph{Separation} between $C_i$ and $C_j$ is calculated according to
Equation~\ref{eq:separation} \citep{hope2013uos}, in which
$\operatorname{proximity}$ is defined as the weight of the co-occurrence in the
inferred sense graph.

\begin{equation}
 \operatorname{separation}(C_i,C_j) =
 1 - \left(
 \frac {\sum_{\substack{x \in C_i \\ y \in C_j}} \operatorname{proximity}(x,y)}
       {|C_i| \times |C_j|}
 \right)
\label{eq:separation}
\end{equation}

The cluster $C_j$ with the lowest separation score compared to $C_i$ is then
considered the most likely sense of the target word $w$.


% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
