\chapter{Evaluation}
\label{ch:eval}

\input{content/evaluation-method.tex}

\section{Test Corpus}

To evaluate the project, the corpus from \citep{baptista2013viper} was used. 
This was a 290K word fragment of the PAROLE corpus \citep{nascimento1998parole}, with each verb instance manually annotated with its ViPEr class and reviewed by linguists. The splitting used for evaluation was to use the whole corpus for training, except for the sentence being evaluated.

\section{Parameter choosing}

The graph partitioning algorithm chosen was \ac{CW}, as the other option,
MaxMax, has a tendency to generate many fine-grained clusters
\citep{hope2013uos}.

The \ac{NPMI} and the logDice association measures were chosen as due to their
normalized scores, which allow to use the same parameter between different words
while keeping the same underlying meaning. The \ac{NPMI} association measure 
was tested with minimum thresholds of 0.0, 0.25, 0.5, and 0.75, ranging from 
each word being at best independent of the other up to both occurring mostly 
together. The logDice association measure was tested with minimum thresholds of 
0.0, 2.5, 5.0, 7.5, and 10.

\section{Unsupervised Evaluation}

The results of the unsupervised evaluation (Table~\ref{tab:unsup-results}) show 
that all tests scored poorly in F-Score, while \ac{MFS} had a result on average 
of 83.4\%. In the V-Measure, results varied from 0.0\% for the lowest values up 
to 34.4\% for the highest. Not counting the most restrictive threshold of 
logDice and NPMI, all tests had better results than \ac{MFS}, which scored 
0.4\% in V-Measure.

Another thing which is possible to see if that not enough points are available
to form a meaningful view of the contexts when the threshold is too high,
resulting in no clusters at all and giving poor results.

When evaluating the number of clusters, it is possible to see that most tests
might have been penalised due to the high number of clusters they had compared
to the average number of \ac{GS} senses.

\begin{table}[ht]
\caption{Results of the unsupervised \ac*{WSI} evaluation.}
\label{tab:unsup-results}
\begin{tabu} to \textwidth {Xlrrrr}
\hline
\textbf{Algorithm} & \textbf{Association Measure} & \textbf{Threshold} & \textbf{F-Score (\%)} & \textbf{V-Measure (\%)} & \textbf{\# Clusters} \\
\hline
\ac{CW} & logDice   &  0.0 &  1.95 & 33.62 & 147.3 \\
\ac{CW} & logDice   &  2.5 &  1.80 & 33.46 & 252.2 \\
\ac{CW} & logDice   &  5.0 &  2.46 & 29.60 & 259.7 \\
\ac{CW} & logDice   &  7.5 &  2.70 & 18.26 &  48.8 \\
\ac{CW} & logDice   & 10.0 &  0.83 &  3.33 &   0.1 \\
\hline
\ac{CW} & \ac{NPMI} & 0.0  &  2.34 & 30.97 &  76.5 \\
\ac{CW} & \ac{NPMI} & 0.25 &  1.69 & 34.37 & 380.1 \\
\ac{CW} & \ac{NPMI} & 0.5  &  0.72 &  9.80 &   0.2 \\
\ac{CW} & \ac{NPMI} & 0.75 &  0.00 &  0.00 &   0.0 \\
\hline
\ac{MFS} &      --- &  --- & 83.36 &  0.37 &   1.0 \\
\hline
\end{tabu}
\end{table}

\section{Supervised Evaluation}

The results on supervised \ac{WSD} (seen in Table~\ref{tab:sup-results}) were
very poor overall. None of the tests were able to surpass the results of
\ac{MFS}, with a precision of 65.7\%. The highest result was using logDice with
a threshold of 7.5, which reached a precision of 10.1\%.

\begin{table}[ht]
\caption{Results of the supervised \ac*{WSD} evaluation.}
\label{tab:sup-results}
\begin{tabu} to \textwidth {Xlrr}
\hline
\textbf{Algorithm} & \textbf{Association Measure} & \textbf{Threshold} & \textbf{Precision (\%)} \\
\hline
\ac{CW} & logDice   &  0.0 &  5.37 \\
\ac{CW} & logDice   &  2.5 &  0.00 \\
\ac{CW} & logDice   &  5.0 &  8.27 \\
\ac{CW} & logDice   &  7.5 & 10.10 \\
\ac{CW} & logDice   & 10.0 &  2.55 \\
\hline
\ac{CW} & \ac{NPMI} & 0.0  &  0.22 \\
\ac{CW} & \ac{NPMI} & 0.25 &  2.65 \\
\ac{CW} & \ac{NPMI} & 0.5  &  0.00 \\
\ac{CW} & \ac{NPMI} & 0.75 &  0.00 \\
\hline
\ac{MFS} & ---      &  --- & 65.74 \\
\hline
\end{tabu}
\end{table}

\section{Results interpretation and evaluation}

Overall, the tests had poor results. In all examples \ac{MFS} was able to
achieve better results, showing the project is not ready to be used for
disambiguation.

The high number of clusters obtained (on average above the hundreds) shows that
the results are too fine-grained to be able to properly match them to the 
senses one is trying to disambiguate.

Further inspection into specific graphs of some words, such as the graph for the
word \emph{vingar} (Figure~\ref{fig:vingar_graph}) can further explain the
obtained results.

\begin{figure}[ht]
  \caption{Image of the induction graph for the word \emph{vingar}, using the
    \ac*{CW} algorithm and the \ac*{NPMI} association measure.}
  \label{fig:vingar_graph}
  \centering
  \includegraphics[width=\textwidth]{graphics/vingar_VERB_npmi_0_4_chineseWhispers}
\end{figure}

The first noticeable thing is that the graph includes a few words with spelling
errors, such as the nodes \emph{natambém} and \emph{estratrégia}. The second 
noticeable thing is that although the work by \citet{correia2015syntax} 
identifies named entities and replaces their lemmas with their categories, many 
of the words in the graph are named entities which were not recognized as such 
by \ac{STRING}. This can be seen in nodes such as \emph{Windsor} and 
\emph{Shrek}, and adds noise to the graph, increasing the number of small 
clusters generated.

But the most noticeable thing is how sparse the graph is. Algorithms such as
\ac{CW} or MaxMax require a \emph{small-world network} with several high-density
areas to be able to find clusters in the graph. In a graph such as the one
in Figure~\ref{fig:vingar_graph}, with the exception of the node corresponding
to the target word, no nodes have more than 2 neighbours. This undermines the
assumptions used in graph-clustering algorithms, and prevents the possibility
of better results.

It is possible the graph is sparse because the syntactic dependencies used
impose a stricter relationship between the two words than the usage of mere
words which co-occur in the same sentence or paragraph would. The stricter
relationship between the words changes the behaviour of the resulting graph,
which make the graph-clustering algorithms behave poorly.

Additionally, the stricter relationship might be preventing words that are
related but do not have a syntactic relationship from being included in the
graph. This might make the generated graphs unsuitable for the specific \ac{WSI}
algorithms used in this project.

Another possible cause of the poor results might be the absence of categories, 
such as \texttt{PERSON}, \texttt{PLACE} or \texttt{ORG}, among others, in the 
algorithm used. As it is blind to the categories, the algorithm can not make 
use of them to help infer the senses of the target word.

% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
