\section{Summary}

Below is a comparison of the various algorithms mentioned in the previous
sections. The algorithms are evaluated according to their performance and to
their features.

\subsection{WSI Algorithms.}

\ac{DSM}-based algorithms have to deal with scalability problems as the number
of contexts increases. Many of them try to deal with the problem by using denser
representations of the context vectors used.

In comparison, graph-based algorithms try to deal with the significance of a
number of co-occurrences, trying to suppress irrelevant co-occurrences as edges
or using smoothing techniques to generate edges, which were not represented in
the original graph.

As it can be seen in Table~\ref{tab:uswsi} and Table~\ref{tab:wsi}, current
implementations of \ac{DSM} and graph-based algorithms have equivalent
performance when compared using either F-score, V-measure or supervised recall.
It is important to note that unless the \ac{GS} and test data used is the same,
the results obtained are not comparable among implementations.

\begin{table}
\caption[Unsupervised evaluation of \acl*{WSI} algorithms]
{Unsupervised evaluation of \ac{WSI} algorithms in nouns. All measures are in
percentage ($\%$). 1c1word, \ac{MFS}, and Random are baselines from each of the
respective datasets. 1c1word and \ac{MFS} groups all instances of a word into a
single cluster.}
\label{tab:uswsi}

\begin{tabu} to \textwidth { Xlrrrrrr }
\hline
\textbf{Algorithm} & \textbf{GS} & \textbf{Prec.} & \textbf{Recall} & \textbf{Purity} & \textbf{Entropy} & \textbf{F-score} & \textbf{V-measure} \\
\hline
CBC               & WordNet        & 60.8 & 50.8 & ---  & ---  & 55.4 & ---  \\
Widdows and Dorow & WordNet        & ---  & ---  & 82.0 & ---  & ---  & ---  \\
Collocations-JC   & \ac{SWSI} 2007 & ---  & ---  & 88.6 & 31.0 & 78.0 & ---  \\
Collocations-BL   & \ac{SWSI} 2007 & ---  & ---  & 89.6 & 29.0 & 73.1 & ---  \\
UoY     & WSI\&D 2010    & ---  & ---  & ---  & ---  & 38.2 & 20.6 \\
HERMIT  & WSI\&D 2010    & ---  & ---  & ---  & ---  & 30.1 & 16.7 \\
\hline
1c1word & \ac{SWSI} 2007 & ---  & ---  & ---  & ---  & 80.7 & \footnotemark[1]0.0 \\
Random  & \ac{SWSI} 2007 & ---  & ---  & ---  & ---  & 38.1 & \footnotemark[1]4.9 \\
MFS     & WSI\&D 2010    & ---  & ---  & ---  & ---  & 57.0 & 0.0  \\
Random  & WSI\&D 2010    & ---  & ---  & ---  & ---  & 30.4 & 4.2  \\
\hline
\end{tabu}
\end{table}

\footnotetext[1]{The mentioned data points were obtained from
\citep{manandhar2009semeval}.}

\begin{table}
\caption[Supervised evaluation of \acl*{WSI} algorithms]
{Supervised evaluation of \ac{WSI} algorithms. Unless otherwise specified, in
the WSI\&D 2010 dataset, the 80-20 split is used.}
\label{tab:wsi}

\begin{tabu} to \textwidth { XXr }
\hline
\textbf{Algorithm} & \textbf{Testing Corpus} & \textbf{Sup. Recall} (\%)\\
\hline
Collocations-JC           & \ac{SWSI} 2007 & 86.4 \\
Collocations-BL           & \ac{SWSI} 2007 & 85.6 \\
UoY            & WSI\&D 2010    & 59.4 \\
HERMIT           & WSI\&D 2010    & 53.6 \\
\hline
MFS           & WSI\&D 2010    & 53.2 \\
Random        & WSI\&D 2010    & 51.5 \\
\hline
\end{tabu}
\end{table}

\subsection{Graph Clustering Algorithms.}

The work in this area introduces algorithms time-linear to the number of edges
with results comparable to older existing algorithms.

Both algorithms are shown to be suitable for \ac{WSI} tasks. Both algorithms are
time-linear to the number of edges and ideal for execution in large-scale graphs
which feature small-world features, as it can be seen on Table~\ref{tab:clalg}.

\ac{CW} is already frequently used in the area of \ac{WSI} with good results,
but unlike MaxMax, it is not deterministic, and it does not allow to place the
same node in several clusters -- soft-clustering --, a feature specially
desirable for a global graph approach so that one word can have several senses.
On the other side, MaxMax is known to generate many fine-grained clusters, which
then need to be merged to obtain clusters more similar in size to the target
senses \citep{hope2013uos}.

\begin{table}
\caption[Comparison of Graph Clustering Algorithms]
{Comparison of Graph Clustering Algorithms. Retrieval Precision (rPrec.),
Retrieval Recall (rRecall), F-score and V-measure are measured in percentage
($\%$).}
\label{tab:clalg}

\begin{tabu} to \textwidth {Xllrrrr}
\hline
\textbf{Alg.} & \textbf{Determ.} & \textbf{Soft-Clust.} & \textbf{rPrec.} & \textbf{rRecall} & \textbf{F-sc.} & \textbf{V-mes.} \\
\hline
\acl*{CW} & No            & No              & 94.8 & 71.3 & ---  & ---  \\
MaxMax      & Yes           & Yes             & ---  & ---  & 13.2 & 32.8 \\
\hline
\end{tabu}
\end{table}

% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
