\section{Other Tools}

In addition to the existing work in \ac{WSD} and \ac{WSI}, it is relevant to
mention other works which make use of word co-occurrences in text.

\subsection{STRING}
\label{sec:string}

\ac{STRING} \citep{mamede2012string} is a \ac{NLP} system for the Portuguese
language, capable of preforming all basic \ac{NLP} tasks, including \ac{NER},
\ac{IR}, \ac{AR}, among other tasks. \ac{STRING} is composed of several stages,
as described in Figure~\ref{fig:stringarch}.

\begin{figure}[ht]
 \caption{\acs*{STRING} Architecture}
 \label{fig:stringarch}
 \centering
 \include{graphics/string-architecture}
\end{figure}

\subsubsection*{Preprocessing}

In the first stage, the text is passed through a tokenizer, which is also
responsible for recognising some special tokens, such as numbers or punctuation.
The tokens are then passed through a \ac{POS} tagger, LexMan
\citep{vicente2013lexman}, which uses finite"~state transducers to label tokens
with their appropriate \ac{POS} tags. At last, the text is split into sentences
mainly based on the punctuation and having into account abbreviations, acronyms,
and the use of ellipsis.

\subsubsection*{POS Disambiguation}

This stage comprises two steps, which are preformed by two distinct modules:

\begin{enumerate}
 \item \ac{RuDriCo2}, which preforms rule-driven morphossyntactic disambiguation
\citep{diniz2010conversor};
 \item \ac{MARv}, which preforms statistical disambiguation
\citep{ribeiro2003anotacao}.
\end{enumerate}

\ac{RuDriCo2} is responsible for refining the initial segmentation done by
LexMan. It uses declarative pattern-matching rules to modify the original
segmentation or disambiguate some of the \ac{POS} tags. \ac{RuDriCo2} also
preforms the expansion of contracted words.

\ac{MARv} is responsible for resolving morphossyntactic ambiguity. It analyses
the labels generated for each word and then chooses the most likely tag given
its immediate context. This is done using \acp{HMM}. \ac{MARv} uses
second-order models, which codify contextual information concerning entities,
and unigrams, which codify lexical information.

\subsubsection*{Syntactic Analysis}

Syntactic analysis is done through the use of \acf{XIP} 
\citep{ait2002robustness},
which also allows for adding lexical, syntactic and semantic information to the
output of the previous modules. \ac{XIP} is responsible for several tasks:

\begin{enumerate}[label=(\roman*)]
 \item It adds information to the existing tokens through the use of lexicons.
\ac{XIP} includes a pre-existing lexicon that can be both extended or modified;
 \item It allows to define local grammars using pattern-matching rules, to group
lexical units together into a single entity;
 \item It also preforms a shallow parsing to group elements into chunks (noun
 phrase -- NP, prepositional phrase -- PP, among others) using linear precedence
 and sequence rules;
 \item Finally it preforms a deep parsing to extract the dependencies between the
different chunks (subject, direct object, modifier, etc.).
\end{enumerate}

\subsubsection*{Post-Syntactic Analysis}

At this stage, additional tasks are executed, which extract additional 
information from the text. One is the \ac{AR} task \citep{marques2013anaphora}, 
which preforms pronominal anaphora identification in the text and then uses the 
\ac{EM} algorithm to obtain the probabilities of each word to be the antecedent 
of a given anaphora candidate. Other tasks performed include time normalisation 
\citep{mauricio2011normalizacao} and event ordering
\citep{cabrita2014ordenar}.

\subsection{Syntax Deep Explorer}

In \citet{correia2015syntax}, a tool was developed making use of \ac{STRING} to
allow one to explore co-occurrence data obtained from Portuguese texts. The
presented solution was composed of a tool to extract co-occurrences and
calculate the association measures of these as well as a web-based interface to
display these co-occurrences in an intuitive fashion.

The developed solution takes advantage of the rich lexical resources available
and the syntactic and semantic analysis of \ac{STRING} to provide information
about the patterns of co-occurrences found in the corpora evaluated.

\subsubsection*{Dependencies and Properties}

For the project in question, each co-occurrence is based on a syntactic
dependency extracted from \ac{XIP}. These dependencies have two words, in which
the first word is the modified element and the second word is the modifier,
information about the dependency itself, and a set of properties, if these
exist. The project only considers a subset of possible properties as 
relevant for its goals, which indicate if the modifier is before or after the 
modified word, or if the modifier is a focus adverb.


\subsubsection*{Architecture}

The implemented solution splits the problem into four separate components:

\begin{itemize}
 \item The storage format of the extracted and computed information;
 \item The co-occurrence extraction from the corpus;
 \item The calculation of the various association measures;
 \item The display of the information in an user-friendly form.
\end{itemize}

\subsubsection*{Storage Format}

An SQLite database was chosen as the format to store the obtained information.
An \ac{ER} model was developed to represent the database, and a relational model
was generated from the created \ac{ER} model.

To store the information in the database, the \ac{ER} model in
Figure~\ref{fig:correira2015er} was created and used. In this model:

\begin{itemize}
	\item The entities \textit{Corpus}, \textit{Word} and \textit{Dependency} 
	are used to store the information about each corpus, word and dependency 
	type respectively.
	
	\item The weak entity \textit{Property} associates the \textit{Dependency} 
	with a property type.
	
	\item Each word has a \textit{Belongs} relation with the corpus, which 
	indicates how often the word occurs in the Corpus.
	
	\item Each pair of words and property also have a \textit{Co-occurrence} 
	relation, with a \textit{frequency} attribute which defines how often these 
	words occur together in the same corpus with the given property. 
	Additionally, several attributes exist for each kind of association measure.
	
	\item The weak entity \textit{Sentence} has a \textit{Belongs} association 
	with the \textit{Corpus}, and the aggregation of the \textit{Co-occurrence} 
	associations has a \textit{Exemplifies} association with the 
	\textit{Sentence} entity.
\end{itemize}

Additionally, the following \acp{IC} were identified:

\begin{enumerate}
  \item The words in the \textit{Co-occurrence} association must belong to the
\textit{Corpus} to which they are associated with.
  \item The \textit{Co-occurrence} association must be associated to the same
\textit{Property} with which the words associate with in \textit{Belongs}.
  \item The sentences in the \textit{Exemplifies} association must belong to the
\textit{Corpus} to which the given \textit{Co-occurrence} is associated with.
\end{enumerate}

\begin{figure}[ht]
\caption[ER model of (Correia et al. 2015)]{The \ac{ER} model used in
	\citep{correia2015syntax}.}
\label{fig:correira2015er}
\centering
\include{graphics/correia2015er}
\end{figure}

\subsubsection*{Co-occurrence extraction}

The co-occurrence extractor obtains the processed \ac{XML} from \ac{XIP} and, 
then, parses it to obtain the dependency information.

The extractor reads all dependencies parsed from the \ac{XML} and stores the 
following information about each of them in the database:

\begin{itemize}
  \item The type of the dependency in question;
  \item The lemma and class of each word in the dependency;
  \item The property of the dependency;
  \item The sentence in which the dependency occurred.
\end{itemize}

\subsubsection*{Association Measures}
\label{sec:assoc}

After the database is populated with the co-occurrences from the 
\textit{corpus}, the association measures are calculated in batches of 2,000 
co-occurrences each.

The measures calculated are:

\begin{itemize}
  \item \acl{PMI};
  \item \textit{Dice} Coefficient;
  \item \textit{LogDice};
  \item Pearson's Chi-Square Test;
  \item Log Likelihood Ratio;
  \item Significance Measure.
\end{itemize}

\subsubsection*{Information Display}

The extracted information, which was stored in the database, is then displayed 
to the user through the use of a web interface written in \ac{PHP} and 
AngularJS. The \emph{front-end}, executed on the \emph{client-side}, makes 
\ac{AJAX} requests to the \emph{back-end}, executed on the \emph{server-side}.

\subsection{Association Measures}

As it could be seen in Section~\ref{sec:vectorspace} and
Section~\ref{sec:co-occurrence}, many works rely in metrics to measure the
cohesion of two words 
\citep{pantel2003clustering,pantel2002discovering,jurgens2010hermit,klapaftis2008word,korkontzelos2010uoy,correia2015syntax}.
This section further examines some association measures and their properties.

\subsubsection*{\acl*{PMI} and \acl*{NPMI}}
\label{subsec:pmi}

\ac{PMI} is a measure of how much the actual probability of a co-occurrence
$p(x,y)$ differs from what would be expected given the probabilities of the
individual events and assuming the independence of $p(x)$ and $p(y)$
\citep{church1990word}. \ac{PMI} is thus defined in Equation~\ref{eq:pmi}.

\begin{equation}
  \label{eq:pmi}
  i(x,y) = \ln \frac{p(x,y)}{p(x)p(y)}
\end{equation}

\ac{NPMI} normalizes the upper and lower bound of \ac{PMI} 
\citep{bouma2009normalized}.
In this case, $i_n(x,y) = 1$ when the two words only occur together,
$i_n(x,y) = 0$ when $x$ and $y$ are independent, and $i_n(x,y)$ approaches $-1$
when the two words occur separately but not together, that is, when $p(x,y)$
approaches 0 and $p(x)$, $p(y)$ are fixed. \ac{NPMI} is thus defined in
Equation~\ref{eq:npmi}.

\begin{equation}
  \label{eq:npmi}
  i_n(x,y) = \left. \left( \ln \frac{p(x,y)}{p(x)p(y)} \right) \middle/
             - \ln p(x,y) \right.
\end{equation}

\subsubsection*{Dice Coefficient and LogDice}
\label{subsec:dice}

The Dice coefficient measures the amount of association between two
co-occurrences \citep{dice1945measures} and ranges from $1.0$, which indicates
association of the two co-occurrences every time they occurred, to $0.0$, which
indicates no association whatsoever between the two co-occurrences under any
time they occurred at all. The Dice coefficient is defined in
Equation~\ref{eq:dice}.

\begin{equation}
  \label{eq:dice}
  D = \frac{2p(x,y)}{p(x) + p(y)}
\end{equation}

\emph{LogDice} addresses the fact that the Dice coefficient usually generates
very small numbers \citep{rychly2008lexicographer}. For \emph{logDice}, the
maximum is $14$. A value of $0$ means there is less than 1 co-occurrence of
$XY$ per 16,000 $X$ or 16,000 $Y$. A value increase in 1 unit means the
co-occurrence occurs twice as often, and a value increase of 7 means the
co-occurrence occurs about 100 times as often. \emph{logDice} is defined in
Equation~\ref{eq:logdice}.

\begin{equation}
  \label{eq:logdice}
  \operatorname{logDice} = 14 + \log_2 D = 14 + \log_2 \frac{2p(x,y)}{p(x) + p(y)}
\end{equation}

\subsubsection*{Pearson's Chi-Squared Test}

$\chi^2$ is a test for dependence which does not assume normally distributed
probabilities \citep{manning1999foundations}. The test compares observed
frequencies with the frequencies expected for independence. In the simplest
case, this test is applied to 2-by-2 tables such as Table~\ref{tab:chi2}. The
$\chi^2$ test sums the differences between the observed and expected values
in all squares of the table, scaled by the magnitude of the expected values,
as per Equation~\ref{eq:chi2}, where $i$ is the table row, $j$ is the table
column, $O_{ij}$ is the observed value of the cell $(i,j)$ and $E_{ij}$ is the
expected value.

\begin{table}[ht]
  \caption{A 2-by-2 table showing the dependence of two words}
  \label{tab:chi2}
  \centering
  \begin{tabu} to \textwidth {| l | r | r |}
    \hline
                 & $w1 = x$ & $w1 \neq x$ \\
    \hline
    $w2 = y$     & $O_{11}$ & $O_{12}$ \\
    \hline
    $w2 \neq y $ & $O_{21}$ & $O_{22}$ \\
    \hline
  \end{tabu}
\end{table}

\begin{equation}
  \label{eq:chi2}
  \chi^2 = \sum_{ij} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
\end{equation}

The expected frequencies $E_{ij}$ are calculated by converting the total of the
rows and columns into proportions. For example, $E_{11}$ is calculated by adding
the items of the first row and dividing it by the sample size ($N$), adding the
items of the first column and dividing it by the sample size, and then
multiplying these two and multiply the result by the sample size, as exemplified
in Equation~\ref{eq:Eij}.

\begin{equation}
  \label{eq:Eij}
  E_{11} = \frac{O_{11} + O_{12}}{N} \times \frac{O_{11} + O_{21}}{N} \times N
\end{equation}

For 2-by-2 tables, the test can be simplified into Equation~\ref{eq:chi22by2}.

\begin{equation}
  \label{eq:chi22by2}
  \chi^2 =  \frac{N(O_{11}O_{22} - O_{12}O_{21})^2}
            {(O_{11} + O_{12}) (O_{11} + O_{21})
             (O_{12} + O_{22}) (O_{21} + O_{22})}
\end{equation}


\subsubsection*{Log Likelihood Ratio}
\label{subsec:loglikeliood}

The Log-likelihood ratio ($G^2$ or $-2\log\lambda$) was created to address the
overestimation of the normal distribution when dealing with very small
probabilities ($np(1 - p) < 5$ for $p$ being the probability that the next word
matches a prototype and $n$ the amount of words for which the match is being
tested) \citep{dunning1993accurate}.

The likelihood ratio tests do not depend on assuming a normal distribution and
instead use the generalized likelihood ratio, which can be used effectively in
smaller volumes of text than is necessary for conventional tests based on
assumed normal distributions \citep{dunning1993accurate}.

For events $k_1,k_2$, the likelihood ratio for the binomial distribution is
defined in Equation~\ref{eq:loglikelihood}.

\begin{equation}
  \label{eq:loglikelihood}
  -2\log\lambda = 2 \sum_{i,j} n_{ij} \log \left( \frac{n_{ij}}{m_{ij}} \right)
  \quad ,\quad
  n_{ij} = \frac{k_{ij}}{k_{i1} + k_{i2}}, m_{ij} = \frac{k_{1j} + k_{2j}}{N}
\end{equation}

\subsubsection*{Significance Measure}

The significance measure is based on the statistical \emph{G-Test} for Poisson
distributions: given two words $A$, $B$, each occurring $a$, $b$ times and $k$
times together, the significance $\operatorname{sig}(A, B)$ of their occurrence
in a sentence is defined in Equation~\ref{eq:sig}, with $n$ being the number of
sentences and $x = \frac{ab}{n}$ \citep{biemann2004language}.

\begin{equation}
  \label{eq:sig}
  \operatorname{sig}(A,B) = x - k \log x = \log k!
\end{equation}


% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
