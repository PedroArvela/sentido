\section{Statistical and Vector Space Models}
\label{sec:vectorspace}

These methods implement \acp{DSM} either based on a statistically or a 
geometrically  oriented probability distribution models \citep{van2010mining}. 
Some of these methods are presented below.

\subsection{\acl*{CBC}}

An implementation of a clustering approach is \ac{CBC} 
\citep{pantel2003clustering}. The algorithm represents each word as a feature 
vector, in which each feature corresponds to a context where the word occurs. 
The value of the feature is the \ac{PMI} between the feature and the word 
\citep{bouma2009normalized}, which is later described in \ref{subsec:pmi}. The 
similarity between two words is computed using the \textit{cosine coefficient} 
\citep{salton1986introduction} of their mutual information vectors, as in 
Equation~\ref{eq:simwiwj}.

\begin{equation}
\operatorname{sim}(w_i,w_j) = \frac{\sum_c mi_{w_i,c} \times mi_{w_j,c}}
                    {\sqrt{\sum_c{mi_{w_i,c}}^2 \times \sum_c{mi_{w_j,c}}^2}}
\label{eq:simwiwj}
\end{equation}

The \ac{CBC} algorithm consists of three phases. In phase I, the top-$k$ 
similar elements of each element $e$ are computed. In phase II a collection of 
tight clusters is constructed, where the elements of each cluster form a 
committee. In each step, the algorithm finds a set of tight clusters, called 
committees, and identifies residue elements not covered by any. First the 
top-$k$ similar elements are clustered using average-link clustering 
\citep{han2000data}. A committee covers an element if its similarity to the 
centroid of the committee exceeds a given threshold. The algorithm then 
recursively attempts to find more committees among the residue elements. The 
details are presented in Algorithm~\ref{alg:cbcphaseii}.

\begin{algorithm}
\begin{algorithmic}
\Function{phaseII}{$E$, $S$, $\theta_1$, $\theta_2$}
  \ForAll {$e \in E$}
    \State $C \gets$ \Call{average-link}{$S$, $e$}
    \ForAll {$c \in C$}
      \Comment{For each cluster discovered}
      \State $Sc(c) \gets |c| \times $ \Call{avgsim}{$c$}
      \Comment{Compute its score}
    \EndFor
    \State $L \gets L \cup c \in C, \max(Sc) = c$
    \Comment{Store highest-scoring cluster in a List L}
  \EndFor

  \State $L \gets$ \Call{sort}{$L$}

  \State $C \gets$ \Call{new-list}{}
  \Comment{List of committees}
  \ForAll {$c \in L$}
    \State $cn \gets$ \Call{centroid}{c}
    \If {$\forall c' \in C, sim(c, $ \Call{centroid}{$c'$} $) < \theta_1$}
      \State $C \gets C \cup c$
    \EndIf
  \EndFor

  \If {$C \subset \emptyset$}
    \State \Return $C$
  \EndIf

  \State $R \gets$ \Call{new-list}{}
  \Comment{List of Residues}
  \ForAll {$e \in E$}
    \If {$\forall c \in C, sim(e, c) < \theta_2$}
      \State $R \gets R \cup e$
    \EndIf
  \EndFor

  \If {$R \subset \emptyset$}
    \State \Return $C$
  \Else
    \State \Return $C \cup$ \Call{phaseII}{$R$, $S$, $\theta_1$, $\theta_2$}
  \EndIf
\EndFunction
\end{algorithmic}
\caption{\label{alg:cbcphaseii} Phase II of CBC}
\end{algorithm}


In phase III each element $e$ is assigned to its most similar clusters
according to Algorithm~\ref{alg:cbcphaseiii}. The similarity between a cluster
and an element is computed using the centroid of the committee members. Once an
element $e$ is assigned to a cluster $c$, the intersecting features between
both are removed from $e$ to allow \ac{CBC} to discover the less frequent
senses of a word and avoid duplicate senses.

\begin{algorithm}
\begin{algorithmic}
\State $C$ is a list of clusters initially empty
\Function{phaseIII}{$e$}
  \State $S \gets$ \Call{top200}{$e$}
  \Comment{The top-200 similar clusters to e}

  \While {$\neg S \subset \emptyset$}
    \State $c \gets$ \Call{most-similar}{$S$, $e$}
    \If {$sim(e, c) < \sigma$}
      \State \textbf{break}
    \EndIf
   \If {$\forall c' \in C, \neg similar(c,c')$}
     \State $c \gets c \cup e$
     \State Remove from e its features that overlap with the features of c
   \EndIf
   \State $S \gets S \setminus \{c\}$
  \EndWhile
  \State \Return $C$
\EndFunction
\end{algorithmic}
\caption{\label{alg:cbcphaseiii} Phase III of CBC}
\end{algorithm}

To evaluate the system, its output was compared with WordNet, with the
frequency counts for the nodes, called synsets, obtained from the SemCor corpus 
\citep{miller1993semantic}.

The corpus was obtained by processing 144 million words of newspaper text from 
the TREC Collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose 
Mercury)\footnote{\url{http://trec.nist.gov/data/test_coll.html}, last 
accessed on \DTMdate{2017-09-28}.}. The test set was constructed by 
intersecting the words in WordNet 
with the nouns in the corpus, resulting in a test set of 13403 words with an 
average number of 740.8 features per word. \ac{CBC} obtained a precision of 
$60.8\%$, a recall of $50.8\%$ and an F-score of $55.4\%$. These evaluation 
measures are later described in Section~\ref{sec:unsupeval}.

\subsection{HERMIT}

The model from \citet{jurgens2010hermit} performs \ac{WSI} by modelling 
individual contexts in a high dimensional word space. Word senses are induced 
by finding contexts which are similar and a hybrid clustering method is used to 
group similar contexts.

Each context of a word is approximated using the \ac{RI} word space model
\citep{kanerva2000random}, in which the occurrence of a word is represented with
an \textit{index vector} instead of a set of dimensions.

\ac{RI} can be described as a two-step operation
\citep{sahlgren2005introduction}:

\begin{enumerate}
 \item Each context in the data is assigned an unique and randomly generated
 representation, called an \textit{index vector}, which is sparse and
 high-dimensional. Its dimensionality ($d$) is on the order of thousands and
 it consists of a small number of randomly distributed values of $\pm1$, with 
 the rest of the elements set to $0$;
 \item Then, context vectors are produced by scanning through the text. Each
 time a word occurs in a context, the context's $d$-dimensional index vector is
 added to the context vector for that word. Words are represented by
 $d$-dimensional context vectors which are the sum of the words' contexts.
\end{enumerate}

This allows for a creation of a co-occurrence matrix $F_{w \times d}$ which is
an approximation of a standard co-occurrence matrix $F_{w \times c}$, but in
which $d \ll c$. As a result, HERMIT transforms the original co-occurrence 
counts into a smaller and denser representation without the computational 
overhead of other dimensional reduction techniques such as \ac{SVD} 
\citep{jurgens2010hermit}.

The identification of related contexts is made through the use of clustering, 
which separates similar context vectors into dissimilar clusters representing 
the distinct senses of a word. A hybrid of the online $k$-means clustering 
\citep{liberty2016algorithm} and \ac{HAC} \citep{zepeda2013hierarchical} with a 
threshold is used. The threshold allows for the number of clusters to be 
determined by data similarity instead of manually specified.

The context vectors are clustered using $k$-means clustering, which assigns a 
context to the most similar cluster centroid. If the nearest centroid has a 
smaller similarity than the \textit{cluster threshold} and there are not any 
$k$ clusters, the context forms a new cluster. The similarity between context 
vectors is defined as the cosine similarity (see above).

Clusters are then repeatedly merged using \ac{HAC} with average link criteria,
that is, cluster similarity is the mean cosine similarity of the pairwise
similarity of all data points from each cluster. When the two most similar
clusters have a similarity less than the threshold, merging stops. The resulting
clusters should then represent distinct word senses.

The model was submitted for SemEval-2010 Task 14 \citep{manandhar2009semeval}
and evaluated in an unsupervised method, which compares the found clusters with 
the gold data for the word senses, and a supervised method, which evaluates the 
results of \ac{WSD} using the induced clusters. The first was measured
according to the F-score and V-measure, which is the harmonic mean between the 
homogeneity and completeness (later described in Section~\ref{sec:unsupeval}); 
while the second was measured using supervised recall. Using the provided test 
corpus, parameters were tuned for a context window size of $\pm1$, a clustering
threshold of $.15$ and a maximum of $15$ clusters per word.

The final results of the SemEval-2010 Task 14 showed that Hermit achieved a
V-measure of $16.7\%$ and an F-score of $24.4\%$ in nouns in the unsupervised
evaluation and a supervised recall of $53.6\%$.


% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
