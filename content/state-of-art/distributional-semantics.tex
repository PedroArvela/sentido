\section{Statistical and Vector Space Models}
\label{sec:vectorspace}

These methods implement \acp{DSM} based on statistically or geometrically  
oriented probability distribution models \citep{van2010mining}. Some of these 
methods are presented below.

\subsection{\acl*{CBC}}

An implementation of a clustering approach is \ac{CBC} 
\citep{pantel2003clustering}. The algorithm represents each word as a feature 
vector, in which each feature corresponds to a context where the word occurs. 
The value of the feature is the \ac{PMI} between the feature and the word 
\citep{bouma2009normalized} (\ac{PMI} is later described in 
Subsection~\ref{subsec:pmi}). The similarity between two words is computed 
using the \textit{cosine coefficient} \citep{salton1986introduction} of their 
mutual information vectors, as in Equation~\ref{eq:simwiwj}.

\begin{equation}
\operatorname{sim}(e_i,e_j) = \frac{\sum_f mi_{e_i,f} \times mi_{e_j,f}}
                    {\sqrt{\sum_f{mi_{e_i,f}}^2 \times \sum_f{mi_{e_j,f}}^2}}
\label{eq:simwiwj}
\end{equation}

The \ac{CBC} algorithm consists of three phases. In phase I, the top-$k$ 
similar elements of each element $e$ are computed. In phase II a collection of
tight clusters is constructed, where the elements of each cluster form a 
committee. In each step, the algorithm finds a set of tight clusters, called 
committees, and identifies residue elements not covered by any. First the 
top-$k$ similar elements are clustered using average-link clustering 
\citep{han2000data}. A committee covers an element if its similarity to the 
centroid of the committee exceeds a given threshold. The algorithm then 
recursively attempts to find more committees among the residue elements. The 
details are presented in Algorithm~\ref{alg:cbcphaseii}.

\begin{algorithm}
\begin{description}[align=left, labelwidth=4em]
\item[Input:] A list of elements $E$ to be clustered, a similarity database $S$
from Phase I, thresholds $\theta_1$ and $\theta_2$. \\

\item[Step 1:] For each element $e \in E$ \\

Cluster the top elements of $e$ from $S$ using average-link clustering
\citep{han2000data}. \\

For each discovered cluster $c$, compute the score
$|c| \times \operatorname{avgsim}(c)$, where $|c|$ is the number of elements in
$c$ and $\operatorname{avgsim}(c)$ is the average pairwise similarity between
elements in $c$. \\

Store the highest-scoring cluster in a list $L$. \\

\item[Step 2:] Sort the clusters in $L$ in descending order of their scores. \\

\item[Step 3:] Let $C$ be a list of committees, initially empty. \\

For each cluster $c \in L$ in sorted order \\

Compute the centroid of $c$ by averaging the feature vectors of its elements and
computing the mutual information scores in the same way as done for individual
elements. \\

If the similarity of $c$ to the centroid of each committee added before to $C$
is below threshold $\theta_1$, add $c$ to $C$. \\

\item[Step 4:] If $C$ is empty, finish and return $C$. \\

\item[Step 5:] For each element $e \in E$ \\

If the similarity of $e$ to every committee in $C$ is below threshold
$\theta_2$, add $e$ to the list of residues $R$. \\

\item[Step 6:] If R is empty, finish and return $C$, otherwise return the union
of $C$ and the output of a recursive call to Phase II using the same input,
except replacing $E$ with $R$. \\

\item[Output:] A list of committees.
\end{description}
\caption[Phase II of CBC]{\label{alg:cbcphaseii} Phase II of CBC  \citep{pantel2003clustering}}
\end{algorithm}


In phase III each element $e$ is assigned to its most similar clusters
according to Algorithm~\ref{alg:cbcphaseiii}. The similarity between a cluster
and an element is computed using the centroid of the committee members. Once an
element $e$ is assigned to a cluster $c$, the intersecting features between
both are removed from $e$ to allow \ac{CBC} to discover the less frequent
senses of a word and avoid duplicate senses.

\begin{algorithm}
\begin{algorithmic}
\State $C$ is a list of clusters initially empty
\Function{phaseIII}{$e$}
  \State $S \gets$ The top-200 similar clusters to $e$

  \While {$\neg (S \subset \emptyset)$}
    \State $c \gets$ The cluster in $S$ most similar to $e$
    \If {$\operatorname{similarity}(e, c) < \sigma$}
      \State \textbf{break}
    \EndIf
   \If {$c$ is not similar to any cluster in $C$}
     \State $e \gets c$
     \State Remove from $e$ its features that overlap with the features of $c$
     \State $C \gets C \cup c$
   \EndIf
   \State $S \gets S \setminus \{c\}$
  \EndWhile
  \State \Return $C$
\EndFunction
\end{algorithmic}
\caption{\label{alg:cbcphaseiii} Phase III of CBC}
\end{algorithm}

To evaluate the system, its output was compared with WordNet, with the
frequency counts for the nodes, called synsets, obtained from the SemCor corpus 
\citep{miller1993semantic}.

The corpus was obtained by processing 144 million words of newspaper text from 
the TREC Collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose 
Mercury)\footnote{\url{http://trec.nist.gov/data/test_coll.html}, last 
accessed on \DTMdate{2017-09-28}.}. The test set was constructed by 
intersecting the words in WordNet 
with the nouns in the corpus, resulting in a test set of 13,403 words with an 
average number of 740.8 features per word. \ac{CBC} obtained a precision of 
60.8\%, a recall of 50.8\% and an F-score of 55.4\%. These evaluation 
measures are later described in Section~\ref{subsec:unsupeval}.

\subsection{HERMIT}

The model from \citet{jurgens2010hermit} performs \ac{WSI} by modelling
individual contexts in a high dimensional word space. Word senses are induced 
by finding contexts which are similar and a hybrid clustering method is used to 
group similar contexts.

Each context of a word is approximated using the \ac{RI} word space model
\citep{kanerva2000random}, in which the occurrence of a word is represented with
an \textit{index vector} instead of a set of dimensions. \ac{RI} can be 
described as a two-step operation \citep{sahlgren2005introduction}:

\begin{enumerate}
 \item Each context in the data is assigned an unique and randomly generated
 representation, called an \textit{index vector}, which is sparse and
 high-dimensional. Its dimensionality ($d$) is on the order of thousands and
 it consists of a small number of randomly distributed values of $\pm1$, with 
 the rest of the elements set to $0$;
 \item Then, context vectors are produced by scanning through the text. Each
 time a word occurs in a context, the context's $d$-dimensional index vector is
 added to the context vector for that word. Words are represented by
 $d$-dimensional context vectors which are the sum of the words' contexts.
\end{enumerate}

This allows for a creation of a co-occurrence matrix $F_{w \times d}$ which is
an approximation of a standard co-occurrence matrix $F_{w \times c}$, but in
which $d \ll c$  ($c$ being the number of contexts). As a result, HERMIT
transforms the original co-occurrence counts into a smaller and denser
representation without the computational overhead of other dimensional reduction techniques such as \ac{SVD} \citep{jurgens2010hermit}.

The identification of related contexts is made through the use of clustering, 
which separates similar context vectors into dissimilar clusters representing 
the distinct senses of a word. A hybrid of the online $k$-means clustering 
\citep{liberty2016algorithm} and \ac{HAC} \citep{zepeda2013hierarchical} with a
threshold is used. The threshold allows for the number of clusters to be 
determined by data similarity instead of manually specified.

The context vectors are clustered using $k$-means clustering, which assigns a 
context to the most similar cluster centroid. If the nearest centroid has a 
smaller similarity than the \textit{cluster threshold} and there are not any 
$k$ clusters, the context forms a new cluster. The similarity between context 
vectors is defined as the cosine similarity (see above).

Clusters are then repeatedly merged using \ac{HAC} with average link criteria,
that is, cluster similarity is the mean cosine similarity of the pairwise
similarity of all data points from each cluster. When the two most similar
clusters have a similarity less than the threshold, merging stops. The resulting
clusters should then represent distinct word senses.

The model was submitted for SemEval-2010 Task 14 \citep{manandhar2009semeval}
and evaluated in an unsupervised method, which compares the found clusters with 
the gold data for the word senses, and a supervised method, which evaluates the 
results of \ac{WSD} using the induced clusters. The first was measured
according to the F"~score and V"~measure. V"~measure is the harmonic mean 
between the \emph{homogeneity} and \emph{completeness} (and will be later 
described in Subsection~\ref{subsec:unsupeval}); while the second metric is 
measured using supervised recall. Using the provided test corpus, parameters 
were tuned for a context window size of $\pm$1, a clustering threshold of 0.15 
and a maximum of 15 clusters per word.

The final results of the SemEval-2010 Task 14 showed that Hermit achieved a
V"~measure of 16.7\% and an F"~score of 24.4\% in nouns in the unsupervised
evaluation and a supervised recall of 53.6\%.


% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
