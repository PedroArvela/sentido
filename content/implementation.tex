\chapter{Implementation}
\label{sec:implementation}

\section{Corpora Pre-Parse}

Two corpora were chosen to be used in this project, the CETEMPúblico corpus
\cite{rocha2000cetempublico}, and a dump of the Portuguese edition Wikipedia
\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Database_download}, last
accessed on \DTMdate{2017-06-21}.}. These were using each their own syntax to
describe their contents. As \ac{STRING} only parses plaintext sentences, the
additional meta-data provided was either removed or adapted for parsing by
\ac{STRING}.

\subsection{CETEMPúblico}

CETEMPúblico was provided in \iac{SGML} format with the following tags:

\begin{description}[labelwidth=3em]
 \item [\texttt{ext}] Extract (usually composed of two paragraphs);
 \item [\texttt{p}]   Paragraph;
 \item [\texttt{s}]   Sentence;
 \item [\texttt{t}]   Title;
 \item [\texttt{a}]   Author;
 \item [\texttt{li}]  List element.
\end{description}

To parse it, a Python script was designed, which parses it as \ac{XML} using a
parser which generates the element tree incrementally. Tags considered
irrelevant were ignored, and tags with special meanings, such as the ones
settings the bounds of a sentence, paragraph or excerpt were replaced with
unique plaintext identifiers.

Because the corpus was in \ac{SGML} format and not \ac{XML}, a few replacements
were made before feeding each line to the parser that made sure that it would
only read valid \ac{XML}. These ensured attributes were quoted and that all
elements had an opening and a closing tag.

\subsection{Wikipedia}

After obtaining the Wikipedia dump, a tool called WikiExtract
\footnote{\url{https://github.com/attardi/wikiextractor}, last accessed on
\DTMdate{2017-06-21}.} was used to convert the obtained XML into mostly plain
text files.

A further cleanup was executed, in which all possible invalid \ac{XML} or
possible leftover tags were found using a regular expression, added to a list,
and removed automatically.

At last, document boundaries and paragraphs were replaced with unique
plaintext identifiers which can be recognized even after being parsed by
\ac{STRING}.

\section{Co-occurrence Extraction}

To obtain the co-occurrences, the extractor created in \cite{correia2015syntax}
was used. The extractor was not able to be used as it was due to different
requirements and conditions for each work, so adaptations were made to it.

\subsection{Storage Model}

To be able to provide all the required information to the graph construction
algorithm, the model used to store the information had to be modified.

A new \ac{ER} was designed, as shown in Figure~\ref{fig:er-model}, and used to
generate the relational model used in the database.

To optimize for the database usage, all tables received their own \texttt{id},
used as the primary key, with the previous primary key being set as a
\texttt{UNIQUE} constraint. The new id primary key is then used to reference to
a given table's line in other tables. This helped reduce the space occupied by
repeated references to text-based primary keys.

\subsection{Parsing XIP files}

The \ac{XIP} files were being parsed as XML using Java's W3C based \ac{DOM}
parser\footnote{\url{https://docs.oracle.com/javase/8/docs/api/org/w3c/dom/package-summary.html},
last accessed on \DTMdate{2017-08-06}}. This parser loads the file in memory and
creates the \ac{DOM}-like tree structure from there.

This implementation was having problems with larger files, on the range of
100MiB and larger, taking exponential amounts of time to preform the most basic
operations.

The existing \ac{DOM} parser was thus replaced with a pull-parser
implementation, which generates events as required to create the new elements,
without having to load the file nor the \ac{DOM} tree in memory.

\subsection{Populating the Database}

After parsing the \ac{XIP} files, the dependencies were extracted using the
tools from \cite{correia2015syntax}. The extracted dependencies were then added
to the database as co-occurrences, by attempting to insert them at first and
updating the existing one if the insertion failed due to uniqueness constraints.
This ensured the number of reads necessary to insert in the database were kept
to a minimum, and thus reduced the amount of time to populate the database.

After all co-occurrences were added to the database, the values of the
association measures were updated in batches of 2000 at a time. To prevent
slowdowns while waiting to read, a cache of read values from the database was
used to prevent reading multiple times the same value from the database,
allowing to considerably reduce the time taken populating the association
measures.

% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
