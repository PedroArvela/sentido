\chapter{Implementation}
\label{ch:implementation}

\section{Corpora Pre-Parse}

Two corpora were chosen to be used in this project, the CETEMPúblico corpus
\citep{rocha2000cetempublico}, and a dump of the Portuguese edition Wikipedia
\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Database_download}, last
accessed on \DTMdate{2017-06-21}.}. Each one was using its own syntax to
describe its contents. As \ac{STRING} only parses plain text sentences, the
additional meta-data provided was either removed or adapted for parsing by
\ac{STRING}.

\subsection{CETEMPúblico}

CETEMPúblico was provided in \iac{SGML} format with the following tags:

\begin{description}[labelwidth=3em]
 \item [\texttt{ext}] Extract (usually composed of two paragraphs);
 \item [\texttt{p}]   Paragraph;
 \item [\texttt{s}]   Sentence;
 \item [\texttt{t}]   Title;
 \item [\texttt{a}]   Author;
 \item [\texttt{li}]  List element.
\end{description}

To parse it, a Python script was designed, which parses it as \ac{XML} using a
parser which generates the element tree incrementally. Tags considered
irrelevant were ignored, and tags with special meanings, such as the ones
settings the bounds of a sentence, paragraph or excerpt were replaced with
unique plain text identifiers.

Because the corpus was in \ac{SGML} format and not \ac{XML}, a few replacements
were made before feeding each line to the parser to made sure that it the parser
only be fed valid \ac{XML}. These ensured attributes were quoted and that all
elements had an opening and closing tag.

\subsection{Wikipedia}

After obtaining the Wikipedia dump, a tool called WikiExtract
\footnote{\url{https://github.com/attardi/wikiextractor}, last accessed on
\DTMdate{2017-06-21}.} was used to convert the obtained \ac{XML} into mostly
plain text files.

Further cleaning was executed, in which all possible invalid \ac{XML} or
possible leftover tags were found using a regular expression, added to a list,
and removed automatically.

At last, document boundaries and paragraphs were replaced with unique
plain text identifiers which can be recognized even after being parsed by
\ac{STRING}.

\section{Co-occurrence Extraction}

To obtain the co-occurrences, the extractor created in \citep{correia2015syntax}
is used. The extractor was not able to be used as it was due to different
requirements and conditions for each work, so adaptations were made to it for
the new environment.

\subsection{Storage Model}

To be able to provide all the required information to the graph construction
algorithm, the model used to store the information had to be modified.

A new \ac{ER} was designed, as shown in Figure~\ref{fig:er-model}, and used to
generate the relational model used in the database.

To optimize for the database usage, all tables have their own \texttt{id},
used as the primary key, with the previous primary key being set as a
\texttt{UNIQUE} constraint. The new \texttt{id} primary key is used to reference
to a given table's line in other tables. This helped reduce the space occupied
by repeated references to text-based primary keys.

\subsection{Parsing \ac{XIP} files}

The \ac{XIP} files were being parsed as XML using Java's W3C based \ac{DOM}
parser\footnote{\url{https://docs.oracle.com/javase/8/docs/api/org/w3c/dom/package-summary.html},
last accessed on \DTMdate{2017-08-06}}. This parser loads the file in memory and
creates the \ac{DOM}-like tree structure from there.

This implementation was having problems with larger files, on the range of
100MiB and larger, taking exponential amounts of time to perform the most basic
operations.

The existing \ac{DOM} parser was thus replaced with a pull-parser. This reads
the file sequentially and, as new tags are found, such as the start or the
end of an element, an event is generated, with only the contents pertaining it.

The pull-parser has an $O(1)$ memory usage for parsing, as only the currently
parsed segment needs to stay in memory as the document structure is built.

On top of the XML pull parser, a basic stack is used to keep track of what is
the current element and depth in the document. The name of the current tag
is pushed to the stack when a start event is emitted, and the top element is
popped when an end event is emitted.

With the events and the stack, a state machine is used, which is responsible for
deciding the next action in the construction of the XIP document in memory.

\subsection{Populating the Database}

After parsing the \ac{XIP} files, the dependencies are extracted using the
tools from \citep{correia2015syntax}. The extracted dependencies are then added
to the database as co-occurrences, by attempting to insert them at first and
updating the existing one if the insertion fail due to uniqueness constraints.

After all co-occurrences are added to the database, the values of the
association measures are updated in batches of 2000 at a time. To prevent
slowdowns while waiting to read, a cache of read values from the database is
used to prevent reading multiple times the same value from the database,
allowing to considerably reduce the time taken populating the association
measures.

\section{Sense Induction}

Given a target word $w$, a query to the database is made to obtain all
co-occurrences which happen in the same context as co-occurrences with $w$ as
either the first or second word, as in Listing~\ref{lst:contextextract}.

\begin{lstlisting}[language=SQL,float=h,caption=SQL Query to extract all
co-occurrences in same context as the target word,label=lst:contextextract]
SELECT Coocorrencia.*,
       p1.palavra AS p1lemma,
       p1.classe AS p1class,
       p2.palavra AS p2lemma,
       p2.classe AS p2class
FROM Coocorrencia
INNER JOIN CoOccurrenceContexts ON Coocorrencia.id = CoOccurrenceContexts.cooccurrence
INNER JOIN Palavra AS p1 ON Coocorrencia.idPalavra1 = p1.idPalavra
INNER JOIN Palavra AS p2 ON Coocorrencia.idPalavra2 = p2.idPalavra
WHERE CoOccurrenceContexts.context IN
    ( SELECT CoOccurrenceContexts.context
     FROM CoOccurrenceContexts
     INNER JOIN Coocorrencia ON CoOccurrenceContexts.cooccurrence = Coocorrencia.id
     WHERE Coocorrencia.idPalavra1 = ?1
       OR Coocorrencia.idPalavra2 = ?1)
\end{lstlisting}

The resulting set of rows consists of all co-occurrences happening in the same
context as $w$. These are then assembled into a graph, in which all the nodes
represent the words from the set, and the edges represent the co-occurrences of
that same set.

All co-occurrences in which the association measure's weight is lower than a
pre-specified threshold are then removed from the graph.

A breadth first search is then applied, starting from $w$, to ensure only nodes
directly connected to $w$ by any number of steps remain in the graph. This
eliminates nodes and co-occurrences not connected to the graph at all, coming
-- for example -- from dangling co-occurrences which were previously connected
only through other co-occurrences which were removed in previous steps of the
generation.

The resulting graph is finally clustered using one of the algorithms explained
in Section~\ref{sec:clusteralgorithms}.

\section{Sense Disambiguation}

To be able to perform disambiguation, additional changes had to be done to the
co-occurrence extractor in \citep{correia2015syntax}. The extractor had the 
logic
to extract the co-occurrences and the code to write them in the database merged
together. To make the extractor usable for the task of disambiguation, a
separation of the code to write in the database and the logic of extracting the
co-occurrences was performed.

Having applied those changes, the disambiguation for a target word $w$ and a
context $c$ starts by using the modified extractor to obtain all co-occurrences
which occur in $c$. These are considered the cluster of co-occurrences of the
word $w$ in context $c$.

To discover which of the induced senses $s$ is the most likely to be in use,
each one of them is compared to the cluster of co-occurrences from the context
using the measure of \emph{separation} defined in Equation~\ref{eq:separation}.
The cluster with the lowest \emph{separation} is considered the most likely
sense of the word in the given context.

\section{Avoiding Duplicated Calculations}

As many of these steps can take considerable amounts of time, it is desirable to
avoid repeating them as often as it is possible. As a result, most of the
execution pipeline of the code was adapted to read and write from files as often
as possible, allowing the project to use these files as a cache for calculated
operations.

The format chosen was a subset of \ac{CSV} files, in which each element was a
row and each property was a column.

Each time an extensive operation is concluded, such as obtaining the
co-occurrences in the context of a word, or generating the clusters for a
word using a given set of parameters, the results are saved in a CSV file.

If that same set of data is required later on, instead of re-calculating it
from scratch, the information is obtained from the existing file.

% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
