\chapter{State of the Art}
\label{sec:stateofart}

In this section the state of the art is described. The overall approaches and
their principles are described, as well as the various algorithms used to
exploit those same principles.

\section{Distributional Semantic Models}

\acp{DSM} discover word senses from text. These are based on the
\textit{Distributional Hypothesis}, which says that \textit{similar terms
appear in similar contexts} \cite{curran2004distributional}. That is, words are
semantically similar if they appear in similar documents, context windows or
syntactic contexts \cite{van2010mining}.

Using syntactic contexts adds a restriction to what can be considered context of
a word. For a word to be in the context of another, it needs to be linked
through the use of a relevant syntactic or lexical relation
\cite{baroni2010distributional}.

\acp{DSM} can then be divided in two types of methods, the ones which implement
semantic similarity based on a statistically oriented probability distribution
model, and the ones which implement semantic similarity based on a geometrically
oriented vector space model \cite{van2010mining}. Some of these methods are
presented below.

\subsection{CBC.}

An implementation of a clustering approach is \ac{CBC}
\cite{pantel2003clustering,pantel2002discovering}. The algorithm represents each
word as a feature vector, in which each feature corresponds to a context where
the word occurs. The value of the feature is the pointwise mutual information
between the feature and the word.

For $F_c(w)$ being the frequency count of a word $w$ occurring in context $c$,
the pointwise mutual information, $mi_{w,c}$, between $c$ and $w$ is defined as
in Equation~\ref{eq:miwc}, where $N = \sum_i\sum_jF_i(j)$ is the total
frequency count of all words and their contexts.

\begin{equation}
 mi_{w,c} = \frac{\frac{F_c(w)}{N}}
                 {\frac{\sum_iF_i(w)}{N} \times
                  \frac{\sum_jF_c(j)}{N}}
 \label{eq:miwc}
\end{equation}

The similarity between two words $w_i$ and $w_j$ is computed using the
\textit{cosine coefficient} of their mutual information vectors, as in
Equation~\ref{eq:simwiwj}.

\begin{equation}
 sim(w_i,w_j) = \frac{\sum_c mi_{w_i,c} \times mi_{w_j,c}}
                     {\sqrt{\sum_c{mi_{w_i,c}}^2 \times \sum_c{mi_{w_j,c}}^2}}
 \label{eq:simwiwj}
\end{equation}

The \ac{CBC} algorithm consists of three phases. In phase I, the top-$k$
similar elements of each element $e$ are computed.

In phase II a collection of tight clusters is constructed, where the
elements of each cluster form a committee. In each step, the algorithm finds a
set of tight clusters, called committees, and identifies residue elements not
covered by any. First the top-$k$ similar elements are clustered using
average-link clustering \cite{han2000data}. A committee covers an element if
its similarity to the centroid of the committee exceeds a given threshold. The
algorithm then recursively attempts to find more committees among the residue
elements. The details are presented in Algorithm~\ref{alg:cbcphaseii}.

\begin{algorithm}
 \begin{algorithmic}
\Function{phaseII}{$E$, $S$, $\theta_1$, $\theta_2$}
  \ForAll {$e \in E$}
    \State $C \gets$ \Call{average-link}{$S$, $e$}
    \ForAll {$c \in C$}
      \Comment{For each cluster discovered}
      \State $Sc(c) \gets |c| \times $ \Call{avgsim}{$c$}
      \Comment{Compute its score}
    \EndFor
    \State $L \gets L \cup c \in C, \max(Sc) = c$
    \Comment{Store highest-scoring cluster in a List L}
  \EndFor

  \State $L \gets$ \Call{sort}{$L$}

  \State $C \gets$ \Call{new-list}{}
  \Comment{List of committees}
  \ForAll {$c \in L$}
    \State $cn \gets$ \Call{centroid}{c}
    \If {$\forall c' \in C, sim(c, $ \Call{centroid}{$c'$} $) < \theta_1$}
      \State $C \gets C \cup c$
    \EndIf
  \EndFor

  \If {$C \subset \emptyset$}
    \State \Return $C$
  \EndIf

  \State $R \gets$ \Call{new-list}{}
  \Comment{List of Residues}
  \ForAll {$e \in E$}
    \If {$\forall c \in C, sim(e, c) < \theta_2$}
      \State $R \gets R \cup e$
    \EndIf
  \EndFor

  \If {$R \subset \emptyset$}
    \State \Return $C$
  \Else
    \State \Return $C \cup$ \Call{phaseII}{$R$, $S$, $\theta_1$, $\theta_2$}
  \EndIf
\EndFunction
 \end{algorithmic}

 \caption{\label{alg:cbcphaseii} Phase II of CBC}
\end{algorithm}


In phase III each element $e$ is assigned to its most similar clusters
according to Algorithm~\ref{alg:cbcphaseiii}. The similarity between a cluster
and an element is computed using the centroid of the committee members. Once an
element $e$ is assigned to a cluster $c$, the intersecting features between
both are removed from $e$ to allow \ac{CBC} to discover the less frequent
senses of a word and avoid duplicate senses.

\begin{algorithm}
 \begin{algorithmic}
  \State $C$ is a list of clusters initially empty
  \Function{phaseIII}{$e$}
    \State $S \gets$ \Call{top200}{$e$}
    \Comment{The top-200 similar clusters to e}

    \While {$\neg S \subset \emptyset$}
      \State $c \gets$ \Call{most-similar}{$S$, $e$}
      \If {$sim(e, c) < \sigma$}
        \State \textbf{break}
      \EndIf
     \If {$\forall c' \in C, \neg similar(c,c')$}
       \State $c \gets c \cup e$
       \State Remove from e its features that overlap with the features of c
     \EndIf
     \State $S \gets S \setminus \{c\}$
    \EndWhile
    \State \Return $C$
  \EndFunction
 \end{algorithmic}

 \caption{\label{alg:cbcphaseiii} Phase III of CBC}
\end{algorithm}

To evaluate the system, its output was compared with WordNet, with the
frequency counts for the nodes, called synsets, obtained from the SemCor corpus.

144M words of newspaper text from the TREC collection were processed to obtain
the corpus. The test set was constructed by intersecting the words in
WordNet with the nouns in the corpus, resulting in a test set of 13403 words
with an average number of 740.8 features per word.

\ac{CBC} obtained a precision of $60.8\%$, a recall of $50.8\%$ and an
F-score of $55.4\%$, outperforming the then existing algorithms by $7.5\%$ on
precision and $5.3\%$ on recall. The F-score is later described in
Section~\ref{sec:unsupeval}.

\subsection{HERMIT.}

The model from \cite{jurgens2010hermit} performs \ac{WSI} by modeling individual
contexts in a high dimensional word space. Word senses are induced by finding
contexts which are similar and a hybrid clustering method is used to group
similar contexts.

Each context of a word is approximated using the \ac{RI} word space model
\cite{kanerva2000random}, in which the occurrence of a word is represented with
an \textit{index vector} instead of a set of dimensions.

\ac{RI} can be described as a two-step operation
\cite{sahlgren2005introduction}:

\begin{enumerate}
 \item Each context in the data is assigned an unique and randomly generated
 representation, called an \textit{index vector}, which is sparse and
 high-dimensional. Their dimensionality ($d$) is on the order of thousands and
 they consist of a small number of randomly distributed $\pm1$, with the rest
 of the elements set to $0$;
 \item Then, context vectors are produced by scanning through the text. Each
 time a word occurs in a context, the context's $d$-dimensional index vector is
 added to the context vector for that word. Words are represented by
 $d$-dimensional context vectors which are the sum of the words' contexts.
\end{enumerate}

This allows for a creation of a co-occurrence matrix $F_{w \times d}$ which is
an approximation of a standard co-occurrence matrix $F_{w \times c}$, but in
which $d \ll c$. As a result, it transforms the original co-occurrence counts
into a smaller and denser representation without the computational overhead of
other dimensional reduction techniques such as \ac{SVD}.

The identification of related contexts is made through the use of clustering,
which separates similar context vectors into dissimilar clusters representing
the distinct senses of a word. A hybrid of the online $k$-means and \ac{HAC}
with a threshold is used. The threshold allows for the number of clusters to be
determined by data similarity instead of manually specified.

The context vectors are clustered using $k$-means, which assigns a context to
the most similar cluster centroid. If the nearest centroid has a similarity
lesser than the \textit{cluster threshold} and there are not $k$ clusters, the
context forms a new cluster. The similarity between context vectors is defined
as the cosine similarity.

Clusters are then repeatedly merged using \ac{HAC} with average link criteria,
that is, cluster similarity is the mean cosine similarity of the pairwise
similarity of all data points from each cluster. When the two most similar
clusters have a similarity less than the threshold, merging stops. The resulting
clusters represent distinct word senses.

The model was submitted for SemEval-2010 Task 14 \cite{manandhar2009semeval}
and evaluated in an unsupervised and a supervised method. The first was measured
according to the V-measure (later described in Section~\ref{sec:unsupeval}) and
F-score, while the second was measured using recall. Using the provided test
corpus, parameters were tuned for a context window size of $\pm1$, a clustering
threshold of $.15$ and a maximum of $15$ clusters per word.

The final results of the SemEval-2010 Task 14 showed that Hermit achieved a
V-measure of $16.7\%$ and an F-score of $24.4\%$ in nouns in the unsupervised
evaluation and a recall of $53.6\%$ in the supervised evaluation.

\section{Co-occurrence Graph Models}

In graph-based models, the meanings of a word are represented by a weighted
and undirected co-occurrence graph. The nodes (or vertices) of the graph are the
words which occur in the corpus and the edges are co-occurrences, with the
weight of the edge describing the amount of times that co-occurrence exists. Two
words are said to co-occur if they both occur within the same context.

These models are based on the idea that co-occurrence graphs have the properties
of \textit{small world networks} \cite{veronis2004hyperlex}, that is, most nodes
are not neighbours to one another, but most nodes can be reached by a small
number of steps, as shown in Figure~\ref{fig:small-world}. These properties
then allow to search for highly interconnected bundles of co-occurrences, that
is, \textit{high density components}, which correspond to the senses being
searched.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{graphics/small_world}
 \caption{An example of a small world network.}
 \label{fig:small-world}
\end{figure}

\subsection{Graph Model for Unsupervised Lexical Acquisition.}

A graph from a \ac{POS}-tagged corpus is made in \cite{widdows2002graph}, using
words as nodes and the grammatical co-occurrence relationships between pairs as
edges. The relationships extracted are the co-occurrences of Noun-Verb,
Verb-Noun, Adjective-Noun, Noun-Noun. To generate the edges the top-$n$
neighbours of each word are selected and turned into edges.

An incremental algorithm is then used to extract categories based on a given
word using affinity scores, which give more importance to words that are linked
to the existing neighbours. The algorithm is tailored to avoid infections from
spurious co-occurrences, preventing spurious links from relating to genuine
semantic similarity. The process to select and add the most similar node to a
set of nodes is described in Algorithm~\ref{alg:similarnodes}, where $A$ is a
set of nodes and $N(a)$ are all the nodes which are linked to a node $a$.

\begin{algorithm}
 \begin{algorithmic}
  \State $N(A) \gets \cup_{a \in A} N(a)$
  \State $b \gets u \in N(A) \setminus A, max(\frac{|N(u) \cap N(A)|}{|N(u)|})$
  \State $A \gets A \cup b$
 \end{algorithmic}
 \caption{\label{alg:similarnodes} Select the most similar node}
\end{algorithm}

The model was built using the \ac{BNC} and evaluated against classes in
WordNet. A WordNet class was considered to be the collection of synsets
subsumed by a parent synset. For a given seed word, the algorithm in
\ref{alg:similarnodes} was used to find the $n$ nodes most closely related to
it. Ten classes were chosen beforehand, and for each class 20 words were
retrieved using a single seed-word from the class in question.

The results show that of a total of 200 retrieved words, only 36 were
incorrect, giving an accuracy of $82\%$.

\subsection{Word Sense Induction Using Graphs of Collocations.}
\label{sec:collocations}

A graph of collocations is used in \cite{klapaftis2008word} to generate a
taxonomy of senses which takes into consideration word polysemy. In this, each
node corresponds to an occurrence of two words in the same context window (which
in the model is a paragraph) and two nodes are connected if two collocations
occur in the same context window.

The base corpus, $bc$, consists of paragraphs containing the target word, $tw$.
Besides $bc$, there is also a large reference corpus, $rc$. The project focuses
in inducing the sense of $tw$ given $bc$ as the only input.

At first, paragraphs with $tw$ are removed from $bc$ and all paragraphs from
$bc$ and $rc$ are \ac{POS} tagged. From these, only nouns are kept and
lemmatised.

Log-likelihood ($G^2$) is then used to filter common nouns which are not
semantically related to $tw$, by checking if a given word $w_i$ has a similar
distribution in $bc$ and $rc$. If that is true, $G^2$ will have a small value
and $w_i$ should be removed from $bc$.

\begin{equation}\label{eq:g2}
 G^2 = 2 * \sum_{i,j} n_{i,j} \cdot log \frac{n_{ij}}{m_{ij}}
\end{equation}

\begin{equation}\label{eq:mij}
 m_{ij} = \frac{\sum_{k=1}^2 n_{ik} \cdot \sum_{k=1}^2 n_{kj}}
               {N}
\end{equation}

The noun frequencies of $bc$ are stored in a list $lbc$ and the noun
frequencies of $rc$ are stored in a list $lrc$.

For each word $w_i \in lbc$ is created a table of observed counts from $lbc$
and $lrc$, OT, and a table of expected values under the model of independence,
ET. $G^2$ is then calculated (Equation~\ref{eq:g2}), where $n_{ij}$ is the
$i,j$ cell of OT and $m_{ij}$ (Equation~\ref{eq:mij}) is the $i,j$ cell of ET,
and $N = \sum_{i,j} n_{ij}$.

$lbc$ is then filtered by removing words with a smaller relative frequency in
$lbc$ in relation to $lrc$. The resulting $lbc$ is then sorted by the $G^2$
values. Words that have a $G^2$ smaller than a pre-specified threshold $p_1$
are then removed from $bc$. By the end of this process, each paragraph in $bc$
is a list of nouns assumed to be topically related to $tw$.

With the base corpus now processed, collocations of two nouns are detected by
generating all $\binom{n}{2}$ combinations for each $n$-length paragraph.
Conditional probabilities (Equation~\ref{eq:pij}) are then used to generate the
weights of each collocation. Collocations that have a frequency and weight
higher than a pre-specified threshold are then used to generate the nodes of the
graph $G$.

\begin{equation}\label{eq:pij}
 p(i|j) = \frac{freq_{ij}}{freq_{j}}
\end{equation}

The constructed graph $G$ is sparse. A smoothing technique is applied to
discover new edges between vertices and to assign weight to all of the graph's
edges. For each vertex $i$, a vertex vector $VC_i$ is assigned containing the
vertices which share an edge with $i$ in $G$. The similarity between each $VC_i$
and $VC_j$ is then calculated using the \ac{JC} (Equation~\ref{eq:jc}).

\begin{equation}\label{eq:jc}
 JC(VC_i, VC_j) = \frac{|VC_i \cap VC_j|}
                       {|VC_i \cup VC_j|}
\end{equation}

The final graph, $G'$, is then clustered using \ac{CW}, further described in
Section~\ref{sec:cw}. This algorithm was used as it does not require input
parameters and performs in linear time to the number of edges, although it is
not guaranteed to converge.

\ac{WSD} is at last made by assigning one of the induced clusters to each
instance of the target word. For each target word in a given paragraph, a score
for each induced cluster is applied, based on the number of collocations which
occur in the paragraph.

For the part of \ac{WSD}, for each paragraph of $bc$, each induced cluster is
assigned a score equal to the number of collocations occurring in it.

To evaluate the model, the framework of the \ac{SWSI} \cite{agirre2007semeval}
was used. The test corpus consists of texts from the Wall Street Journal
corpus, hand-tagged with OntoNotes senses \cite{hovy2006ontonotes}.

The model was evaluated under \textit{unsupervised evaluation}, where the
resulting classes are compared to the \ac{GS}, and under \textit{supervised
evaluation}, where the model's performance on a \ac{WSD} setting with the
test corpus is measured.

The model was evaluated with and without smoothing. In the unsupervised
evaluation, the model with smoothing achieved $88.6\%$ purity, $31\%$ entropy
(these measures are described below, in Section~\ref{sec:unsupeval}) and an
F-Score of $78.0\%$. Results without smoothing were similar, but with a lower
F-Score due to the larger number of induced clusters.

\subsection{UoY: Graphs of Unambiguous Vertices.}

The model developed in \cite{korkontzelos2010uoy} is a relaxed version of the
model in \cite{klapaftis2008word}, described in Section~\ref{sec:collocations},
in which a node is generated from a single word if it is considered unambiguous,
and a node is only generated from a set of two words if otherwise.

The corpus is first preprocessed, with the aim of capturing words contextually
related to the target. Sentences or paragraphs, \textit{snippets}, which contain
the target word are lemmatised and \ac{POS} tagged using the GENIA tagger. Only
nouns are kept and words which occour in a stoplist are filtered out. Nouns
which are infrequent in the reference corpus are removed and the log-likelihood
ratio ($G^2$) is used to compare the distribution of each noun to its
distribution in the reference corpus. If a noun's $G^2$ is lower than a
specified threshold, or the noun has a higher relative frequency in the
reference corpus than in the target corpus, then that noun is removed. At this
stage, each snippet is a list of lemmatised nouns contextually related to the
target word.

The graph is constructed by first representing all nouns in the list as graph
vertices. Each noun within a snippet is combined with every other, generating
$\binom{n}{2}$ pairs. $G^2$ is applied once again, to the pairs to filter out
unimportant ones.

To filter out pairs which refer to the same sense, a vector with the snippet IDs
in which they occur is generated for each pair and each noun. A pair is
discarded if its vector is similar to both vectors of their component nouns,
using for that purpose the Dice coefficient defined in Equation~\ref{eq:dice}.

\begin{equation} \label{eq:dice}
 QS = \frac{2 |X \cap Y|}{|X| + |Y|}
\end{equation}

Edges are drawn based on the co-occurrence of the corresponding vertices in
snippets. The weight of the edge is the maximum of the conditional probabilities
of its vertices, calculated according to Equation~\ref{eq:wab}, and low weighted
edges are filtered out.

\begin{equation} \label{eq:wab}
 w_{a,b} = \frac{1}{2} \left( \frac{f_{a,b}}{f_a} + \frac{f_{a,b}}{f_b} \right)
\end{equation}

The graph is then clustered using \ac{CW}, described in Section~\ref{sec:cw}.
To reduce the number of clusters, a post-processing stage is applied in which
for each cluster $l_i$, a set of all snippets $S_i$ containing at least one
vertex of $l_i$ is generated. For any clusters $l_a$ and $l_b$, if
$S_a \subseteq S_b$  or $S_a \supseteq S_b$, these are merged.

The model was submitted for SemEval-2010 Task 14 \cite{manandhar2009semeval}
and evaluated in an unsupervised and a supervised method. The first was measured
according to the V-measure and F-score, while the second was measured using
recall. Parameters were tuned by choosing maximum supervised recall, resulting
in threshold frequencies of 10, $G^2$ threshold of 10, collocations weights of
0.4 and similarity threshold for pairs-of-nouns vertices of 0.8.

Results showed that the model achieved results of a V-measure of $20.6\%$, an
F-score of $38.2\%$ on the unsupervised evaluation with nouns and a recall of
$59.4$ on the supervised evaluation.

\section{Graph Partitioning and Clustering Algorithms}

The task of finding clusters which are optimal with respect to fitness measures
is NP-complete \cite{sima2006np}. The following are two current algorithms,
time-linear to the number of edges, which try to find solutions through
approximation.

\subsection{Chinese Whispers.}
\label{sec:cw}

\ac{CW} is a randomised graph-clustering algorithm which is time-linear in the
number of edges, developed by Biemann \cite{biemann2006chinese}. It is a basic,
yet effective, algorithm to partition nodes of weighted, undirected graphs, and
it is said to perform well in small-world graphs.

\ac{CW} is parameter-free, as the number of partitions emerges naturally during
the process. But formally, \ac{CW} does not converge, as a node can become tied
and be randomly assigned a different class at each iteration, without ever
stabilising, nor is it deterministic, due to the random orders and assignments
in the algorithm.

In \ac{CW}, a weighted graph, $G=(V,E)$, has nodes $v_i \in V$ and weighted
edges $(v_i, v_j, w_{ij}) \in E$ with weight $w_{ij}$. If $(v_i, v_j, w_{ij})
\in E$ implies $(v_j, v_i, w_{ij}) \in E$ then $G$ is undirected. If all weights
are 1, $G$ is unweighted. The degree of a node is the number of edges it takes
part in. The neighbourhood of a node $v$ is the set of all nodes $v'$ such that
$(v,v',w) \in E$ or $(v',v,w) \in E$.

\ac{CW} works as outlined in Algorithm~\ref{alg:cw}. First, all nodes get
different classes. For a small number of iterations, in each iteration, the
nodes inherit the strongest class in their neighbourhood. This is the class
whose sum of edge weights to the current node is maximal. If multiple classes
are equally the strongest, one is chosen randomly. Classes are updated
immediately, a node can obtain classes introduced in that same iteration.

Regions of the same class stabilize during the iteration, and grow until they
reach the border of another class.

\begin{algorithm}
 \begin{algorithmic}
  \Function{ChineseWhispers}{V, E}
   \ForAll{$v_i \in V$}
    \State $class(v_i) \gets i$
   \EndFor

   \While{changes}
    \ForAll{$v \in V$, random order}
     \State $class(v) \gets max\_rank(class(neighbourhood(v)))$
    \EndFor
   \EndWhile
  \EndFunction
 \end{algorithmic}
 \caption{\label{alg:cw} The Chinese Whispers algorithm}
\end{algorithm}

Apart from ties, the class of a node usually does not change more than a few
times. The number of iterations depends on the larger distance between two nodes
in the graph.

\ac{CW} was evaluated on tasks of \ac{WSI} using a similar approach to the one
in \cite{dorow2003discovering}, by replacing the Markov Clustering algorithm
with \ac{CW}. The evaluation method in \cite{bordag2006word} was used. In this,
the neighbourhood of two words is merged and the ability of the algorithm to
separate the merged graph is evaluated. The evaluation measures used included
\textit{retrieval precision} ($rP$), the similarity of the found sense with the
gold standard sense using the overlap measure, and the \textit{retrieval recall
($rR$)}, which are the amount of words assigned correctly to the gold standard
sense.

Results for nouns showed that \ac{CW} had a retrieval precision of $94.8\%$ and
a retrieval recall of $71.3\%$, which suggest similar performance as specialized
graph-clustering algorithms for \ac{WSI} given the same input.

\subsection{MaxMax.}
\label{sec:maxmax}

MaxMax is a soft-clustering algorithm applicable to edge-weighted graphs
\cite{hope2013maxmax}. It is parameter-free, runs in linear time to the number
of edges and it is deterministic. Test results show it to return scores
comparable with existing state-of-the-art systems.

In MaxMax, a notion of \textit{maximal affinity} is used, in which affinity
between vertices $u$ and $v$ is the edge weight $w(u,v)$. A vertex $u$ has
maximal affinity to a vertex $v$ if $w(u,v)$ is maximal among all edges with
$u$. $v$ is said to be a \textit{maximal vertex of $u$}.

MaxMax consists of two stages, as described in Algorithm~\ref{alg:maxmax}.
First, the weighted graph $G$ is transformed in an unweighted, directed graph
$G'$. Maximal affinity relationships between vertices are used to determine edge
direction in $G'$. If in $G$ a vertex $u$ has two maximal vertexes $v$ and $w$,
in $G'$ $u$ will have only two directed edges, from $v$ to $u$ and from $w$ to
$u$.

Then, clusters are identified, finding the \textit{root} vertices of subgraphs
of $G'$ and by marking all descendants of a vertex as $\neg root$. In the
directed graph $G'$, a vertex $v$ is a \textit{descendant} of $u$ is there is a
directed path from $u$ to $v$. At the end of the stage, vertices which are still
marked as $root$ uniquely identify clusters.

\begin{algorithm}
 \begin{algorithmic}
  \Function{MaxMax}{V, E}
   \ForAll{$(u,v) \in E$}
    \If{$v$ is $maximal\_vertex(u)$}
     \State $E' \gets E' \cup (v,u)$
    \EndIf
   \EndFor
   \State $G' \gets dirgraph(V, E')$
   \State $\forall v \in V, v \gets root$

   \ForAll{$v \in V$}
    \If{$v$ is $root$}
     \ForAll{$u \in descentant(v)$}
      \State $u \gets \neg root$
     \EndFor
    \EndIf
   \EndFor
  \EndFunction
 \end{algorithmic}
 \caption{\label{alg:maxmax} The MaxMax algorithm}
\end{algorithm}

MaxMax was evaluated in the context of the SemEval 2010 \ac{WSI} Task
\cite{manandhar2010semeval}, using an adaptation of the Shared Nearest
Neighbours algorithm and then using MaxMax to identify sense clusters in the
generated target word graph. MaxMax has the best scoring V-measure (of $32.8\%$)
among the systems evaluated in \cite{manandhar2010semeval}, and the worst
F-score (of $13.2\%$) among the systems evaluated. The authors claim that their
system is overly penalized by the F-score due to the way this is known to be
biased towards clustering solutions returning large clusters and to punish small
clusters disproportionately.

\section{Summary}

Below is a comparison of the various algorithms mentioned in the previous
sections. The algorithms are evaluated according to their performance and to
their features.

\subsection{WSI Algorithms.}

\ac{DSM}-based algorithms have to deal with scalability problems as the number
of contexts increases. Many of them try to deal with the problem by using denser
representations of the context vectors used.

In comparison, graph-based algorithms try to deal with the significance of a
number of co-occurrences, trying to suppress irrelevant co-occurrences as edges
or using smoothing techniques to generate edges, which were not represented in
the original graph.

As it can be seen in Table~\ref{tab:uswsi} and Table~\ref{tab:wsi}, current
implementations of \ac{DSM} and graph-based algorithms have equivalent
performance when compared using either F-score, V-measure or supervised recall.
It is important to note that unless the \ac{GS} and test data used is the same,
the results obtained are not comparable among implementations.

\begin{table}
\centering
\caption{\label{tab:uswsi} Unsupervised evaluation of \ac{WSI} algorithms in
nouns. All measures are in percentage ($\%$). 1c1word, \ac{MFS}, and Random are
baselines from each of the respective datasets. 1c1word and \ac{MFS} groups all
instances of a word into a single cluster.}

\begin{tabu} to \textwidth { XXrrrrrr }
\hline
\textbf{Algorithm} & \textbf{GS} & \textbf{Prec.} & \textbf{Recall} & \textbf{Purity} & \textbf{Entropy} & \textbf{F-score} & \textbf{V-measure} \\
\hline
CBC \cite{pantel2002discovering}          & WordNet        & 60.8 & 50.8 & ---  & ---  & 55.4 & ---  \\
WD \cite{widdows2002graph}                & WordNet        & ---  & ---  & 82.0 & ---  & ---  & ---  \\
Col-JC \cite{klapaftis2008word}           & \ac{SWSI} 2007 & ---  & ---  & 88.6 & 31.0 & 78.0 & ---  \\
Col-BL \cite{klapaftis2008word}           & \ac{SWSI} 2007 & ---  & ---  & 89.6 & 29.0 & 73.1 & ---  \\
UoY \cite{korkontzelos2010uoy}            & WSI\&D 2010    & ---  & ---  & ---  & ---  & 38.2 & 20.6 \\
HERMIT \cite{jurgens2010hermit}           & WSI\&D 2010    & ---  & ---  & ---  & ---  & 30.1 & 16.7 \\
\hline
1c1word \cite{agirre2007semeval}          & \ac{SWSI} 2007 & ---  & ---  & ---  & ---  & 80.7 & \footnotemark[1]0.0 \\
Random \cite{agirre2007semeval}           & \ac{SWSI} 2007 & ---  & ---  & ---  & ---  & 38.1 & \footnotemark[1]4.9 \\
MFS \cite{manandhar2010semeval}           & WSI\&D 2010    & ---  & ---  & ---  & ---  & 57.0 & 0.0  \\
Random \cite{manandhar2010semeval}        & WSI\&D 2010    & ---  & ---  & ---  & ---  & 30.4 & 4.2  \\
\hline
\end{tabu}
\end{table}

\footnotetext[1]{The mentioned data points were obtained from
\cite{manandhar2009semeval}.}

\begin{table}
\centering
\caption{\label{tab:wsi} Supervised evaluation of \ac{WSI} algorithms. Unless
otherwise specified, in the WSI\&D 2010 dataset, the 80-20 split is used.}

\begin{tabu} to \textwidth { XXr }
\hline
\textbf{Algorithm} & \textbf{Testing Corpus} & \textbf{Recall} (\%)\\
\hline
Col-JC \cite{klapaftis2008word}           & \ac{SWSI} 2007 & 86.4 \\
Col-BL \cite{klapaftis2008word}           & \ac{SWSI} 2007 & 85.6 \\
UoY \cite{korkontzelos2010uoy}            & WSI\&D 2010    & 59.4 \\
HERMIT \cite{jurgens2010hermit}           & WSI\&D 2010    & 53.6 \\
\hline
MFS \cite{manandhar2010semeval}           & WSI\&D 2010    & 53.2 \\
Random \cite{manandhar2010semeval}        & WSI\&D 2010    & 51.5 \\
\hline
\end{tabu}
\end{table}

\subsection{Graph Clustering Algorithms.}

The work in this area introduces algorithms time-linear to the number of edges
with results comparable to older existing algorithms.

Both algorithms are shown to be suitable for \ac{WSI} tasks. Both algorithms are
time-linear to the number of edges and ideal for execution in large-scale graphs
which feature small-world features, as it can be seen on Table~\ref{tab:clalg}.

\ac{CW} is already frequently used in the area of \ac{WSI} with good results,
but unlike MaxMax, it is not deterministic, and it does not allow to place the
same node in several clusters -- soft-clustering --, a feature specially
desirable for a global graph approach so that one word can have several senses.

\begin{table}
\centering
\caption{\label{tab:clalg} Comparison of Graph Clustering Algorithms. Retrieval
Precision (rPrec.), Retrieval Recall (rRecall), F-score and V-measure are
measured in percentage ($\%$).}

\begin{tabu} to \textwidth {XXXrrrr}
\hline
\textbf{Alg.} & \textbf{Determ.} & \textbf{Soft-Clust.} & \textbf{rPrec.} & \textbf{rRecall} & \textbf{F-sc.} & \textbf{V-mes.} \\
\hline
\ac{CW} \cite{biemann2006chinese} & No            & No              & 94.8 & 71.3 & ---  & ---  \\
MaxMax \cite{hope2013maxmax}      & Yes           & Yes             & ---  & ---  & 13.2 & 32.8 \\
\hline
\end{tabu}
\end{table}
% kate: default-dictionary en_GB; indent-width 2; replace-tabs on;
% kate: remove-trailing-space on; space-indent on;
% kate: replace-trailing-space-save on; remove-trailing-space on;
